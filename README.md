# Text_Simplification_Using_Bert_And_GPT2
This project implements a **Text Simplification Model** using a **BERT Encoder** and **GPT-2 Decoder** with Hugging Faceâ€™s `transformers` library.  
It takes a complex sentence as input and generates a simplified version of it.

text-simplification/
â”‚
â”œâ”€â”€ data/                          # Dataset files
â”‚   â”œâ”€â”€ src_train.txt              # Complex sentences
â”‚   â”œâ”€â”€ tgt_train.txt              # Simplified sentences
â”‚
â”œâ”€â”€ src/                           # Core source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py             # Dataset loading
â”‚   â”œâ”€â”€ dataset_split.py           # Train/Val/Test split
â”‚   â”œâ”€â”€ tokenization.py            # Tokenization logic
â”‚   â”œâ”€â”€ model_setup.py             # Model and tokenizer setup
â”‚   â”œâ”€â”€ train.py                   # (Optional) training loop
â”‚   â”œâ”€â”€ evaluate.py                # (Optional) evaluation
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ model.ipynb                # Original Jupyter notebook
â”‚
â”œâ”€â”€ main.py                        # Orchestrates full pipeline
â”œâ”€â”€ requirements.txt               # Dependencies
â””â”€â”€ README.md                      # Project documentation

âš™ï¸ Features
ğŸ“‘ Load parallel datasets (complex â†’ simplified sentences)
ğŸ”€ Split data into train / validation / test
ğŸ¤– Prepare Hugging Face DatasetDict
ğŸ”§ Fine-tune a BERT-GPT2 Encoder-Decoder model
âœ‚ï¸ Tokenize complex/simplified text pairs
ğŸ§ª Run inference and generate simplified sentences
